# Chapter 13: Distributed Transactions

## Atomic Commitment

In databases, we often need to execute multiple operations atomically. The main focus is on making operations appear indivisible and permanent. In this case, transactions which execute prior to that have their history preserved.

Transaction atomicity implies that all its results become visible. If the transaction cannot complete, its results have to be rolled back. To make multiple operations appear atomic, if some of them are remote, we need to use a class of algorithms called atomic commitment.

Atomic commitment doesn't allow disagreements between the participants. A transaction will not commit if even one of the participants vote against it.

## Two-Phase Commit (2PC)

Let's start with the most straightforward protocol for distributed commitment: Two-Phase Commit (2PC). 2PC is usually implemented with the following structure:

2PC executes in two phases:

1. During the first phase, the decided value is distributed and votes are collected
2. During the second phase, nodes just flip the switch, making the results of the first phase visible

2PC assumes the presence of a leader or coordinator that holds the state, collects votes, and is a primary point of reference for the agreement round. The rest of the nodes are called cohorts. Cohorts are usually partitions that operate over disjoint datasets, against which transactions execute.

### Failure Scenarios in 2PC

Let's consider several failures:

- If one of the cohorts is unavailable, the coordinator will abort the transaction. This requirement has a negative impact on availability.
- Failure of a single node can prevent progress. Some systems, for example Spanner, perform 2PC over Paxos groups rather than individual nodes to improve protocol availability.

The main idea behind 2PC is a promise by a cohort that once it has positively responded to the proposal, it will not go back on its decision. So only the coordinator can abort the transaction.

If one of the cohorts has failed after accepting the proposal, it has to learn about the actual outcome of the vote because it can't serve values correctly since the coordinator might have aborted the commit due to the other cohort's decision.

Inability of the coordinator to proceed with the commit or the abort leaves the cluster in an undecided state. This means that cohorts will not be able to learn about final decision in case of a permanent coordinator failure. Because of this property, we say that 2PC is a blocking atomic commitment algorithm.

If the coordinator never recovers, its replacement has to collect votes for a given transaction again, preceded by the final decision.

Many databases use 2PC: MySQL, PostgreSQL, MongoDB, and others. 2PC is often used to implement distributed transactions because of its simplicity and low overhead.

## Three-Phase Commit (3PC)

To make an atomic commitment protocol robust against coordinator failures and avoid undecided states, the three-phase commit protocol has an extra step and timeouts on both sides that can allow cohorts to proceed with either commit or abort in the event of coordinator failure, depending on the system state.

3PC adds a prepare phase before the commit/abort step, which communicates cohorts' states collected by the coordinator during the propose phase, allowing the protocol to carry on even if the coordinator fails.

Steps in 3PC:

1. **Propose**: The coordinator sends out a proposed value and collects the votes.
2. **Prepare**: The coordinator notifies cohorts about the vote results. If the vote has passed and all cohorts have decided to commit, the coordinator sends a prepare message instructing them to prepare to commit. Otherwise, an abort message is sent and the round completes.
3. **Commit**: Cohorts are notified by the coordinator to commit the transaction.

## Reducing Contention

There are other ways to reduce contention and the total amount of time during which transactions hold locks. One way is to let replicas agree on the execution order and transaction boundaries before acquiring locks and proceeding with the execution.

If we can achieve this, node failure should not cause transaction abort, since nodes can recover state from other participants that execute the same transaction in parallel.

Traditional database systems execute transactions using two-phase locking or optimistic concurrency control and have no deterministic transaction order. This means that nodes have to be coordinated to preserve order.

Deterministic transaction order removes coordination overhead during the execution phase and, since all replicas get the same inputs, they also produce equivalent outputs. This approach is commonly known as Calvin.

## Calvin: Fast Distributed Transaction Protocol

One of the prominent examples implementing distributed transactions using Calvin is FaunaDB.

Calvin is often contrasted with another approach for distributed transaction management called Spanner. Its implementations include several open source databases, most prominently CockroachDB and YugaByteDB.

While Calvin establishes the global transaction execution order by reaching consensus on sequencers, Spanner uses two-phase commit over consensus groups per partition.

Spanner has a rather complex setup, and we only cover high-level details in the scope of this project.

## Partitioning

Many databases use partitioning, a logical division of data into smaller, manageable segments. The most straightforward way to partition data is by splitting it into ranges and allowing replica sets to manage only specific ranges.

When executing queries, clients have to route requests based on the routing key to the correct replica set for read/write operations. This partitioning scheme is typically called sharding. Every replica set acts as a single source of truth for a subset of data.

To use partitions most effectively, they have to be sized taking the load and value distribution into consideration. When nodes are added or removed from the cluster, the database has to repartition the data to maintain balance.

Some databases perform auto-sharding and relocate the data using placement algorithms that determine optimal partitioning. These algorithms use information about read/write loads and amount of data in each shard.

## Consistent Hashing

To find a target node from the routing key, some database systems compute a hash of the key and use it for mapping from the hash value to the node. One of the advantages of using hash functions to determine replica placement is that it can help to reduce range hotspotting, since hash values do not sort the same way as the original values.

The most straightforward way to map hash values to node_i is by taking the remainder of the division of the hash value by the size of the cluster. The main problem with this approach is that whenever nodes are added or removed, the hash values will differ from the original ones.

In order to mitigate this problem, consistent hashing is used. It's a different partitioning approach where values returned by the hash function map to a ring, so that after the largest possible value, it wraps around to its smallest value. Each node gets its own position on the ring and becomes responsible for a range of values.
